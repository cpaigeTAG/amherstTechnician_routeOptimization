{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import pyodbc as odbc\n",
    "from k_means_constrained import KMeansConstrained\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Data From SQL\n",
    "#### Property Data: ModelTestBed..msrportfolio\n",
    "#### Tech Data: IRSPublish..tv_technicianAddress"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "Driver_name = 'SQL Server Native Client 11.0'\n",
    "Server_Name = 'DFILSQL02.insightlabs.amherst.com'\n",
    "Database_Name = 'IRSPublish'\n",
    "\n",
    "\n",
    "cnxn = odbc.connect(\"Driver={SQL Server Native Client 11.0};\"\n",
    "                    \"Server=DFILSQL02.insightlabs.amherst.com;\"\n",
    "                    \"Trusted_Connection=yes;\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpaige\\AppData\\Local\\Temp\\ipykernel_3620\\2299689251.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  raw_prop_data = pd.io.sql.read_sql(query_lat_long_data,cnxn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT [PropertyID] as propertyid\n      ,[BranchName] as branchname\n      ,[State] as state\n      ,[CensusTractGeoId]\n      ,[latitude] as property_lat\n      ,[longitude] as property_long\n      ,[SUBDIVISION_NAME] as subdivision_name\n  FROM [ModelTestBed].[dbo].[MSRportfolio]\n  where [Still Under Mgmt]='Yes'\n  and BranchName like '%st louis%'\n': ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'SUBDIVISION_NAME'. (207) (SQLExecDirectW)\")",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mProgrammingError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2018\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2017\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2018\u001B[0m     cur\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cur\n",
      "\u001B[1;31mProgrammingError\u001B[0m: ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'SUBDIVISION_NAME'. (207) (SQLExecDirectW)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDatabaseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [10], line 14\u001B[0m\n\u001B[0;32m      1\u001B[0m query_lat_long_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'''\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124mSELECT [PropertyID] as propertyid\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124m      ,[BranchName] as branchname\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;124m  and BranchName like \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mt louis\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124m'''\u001B[39m\n\u001B[1;32m---> 14\u001B[0m raw_prop_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_lat_long_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcnxn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m lat_long_df \u001B[38;5;241m=\u001B[39m raw_prop_data\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# lat_long_df = lat_long_df[lat_long_df['state'] == 'MO']\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:564\u001B[0m, in \u001B[0;36mread_sql\u001B[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001B[0m\n\u001B[0;32m    561\u001B[0m pandas_sql \u001B[38;5;241m=\u001B[39m pandasSQL_builder(con)\n\u001B[0;32m    563\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pandas_sql, SQLiteDatabase):\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpandas_sql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_query\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43msql\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_float\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    574\u001B[0m     _is_table_name \u001B[38;5;241m=\u001B[39m pandas_sql\u001B[38;5;241m.\u001B[39mhas_table(sql)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2078\u001B[0m, in \u001B[0;36mSQLiteDatabase.read_query\u001B[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001B[0m\n\u001B[0;32m   2066\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_query\u001B[39m(\n\u001B[0;32m   2067\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2068\u001B[0m     sql,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2074\u001B[0m     dtype: DtypeArg \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2075\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Iterator[DataFrame]:\n\u001B[0;32m   2077\u001B[0m     args \u001B[38;5;241m=\u001B[39m _convert_params(sql, params)\n\u001B[1;32m-> 2078\u001B[0m     cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2079\u001B[0m     columns \u001B[38;5;241m=\u001B[39m [col_desc[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m col_desc \u001B[38;5;129;01min\u001B[39;00m cursor\u001B[38;5;241m.\u001B[39mdescription]\n\u001B[0;32m   2081\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2030\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2027\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minner_exc\u001B[39;00m\n\u001B[0;32m   2029\u001B[0m ex \u001B[38;5;241m=\u001B[39m DatabaseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecution failed on sql \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2030\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mDatabaseError\u001B[0m: Execution failed on sql '\nSELECT [PropertyID] as propertyid\n      ,[BranchName] as branchname\n      ,[State] as state\n      ,[CensusTractGeoId]\n      ,[latitude] as property_lat\n      ,[longitude] as property_long\n      ,[SUBDIVISION_NAME] as subdivision_name\n  FROM [ModelTestBed].[dbo].[MSRportfolio]\n  where [Still Under Mgmt]='Yes'\n  and BranchName like '%st louis%'\n': ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'SUBDIVISION_NAME'. (207) (SQLExecDirectW)\")"
     ]
    }
   ],
   "source": [
    "query_lat_long_data = '''\n",
    "SELECT [PropertyID] as propertyid\n",
    "      ,[BranchName] as branchname\n",
    "      ,[State] as state\n",
    "      ,[CensusTractGeoId]\n",
    "      ,[latitude] as property_lat\n",
    "      ,[longitude] as property_long\n",
    "      ,[SUBDIVISION_NAME] as subdivision_name\n",
    "  FROM [ModelTestBed].[dbo].[MSRportfolio]\n",
    "  where [Still Under Mgmt]='Yes'\n",
    "  and BranchName like '%st louis%'\n",
    "'''\n",
    "\n",
    "raw_prop_data = pd.io.sql.read_sql(query_lat_long_data,cnxn)\n",
    "\n",
    "lat_long_df = raw_prop_data.copy()\n",
    "# lat_long_df = lat_long_df[lat_long_df['state'] == 'MO']\n",
    "lat_long_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat_long_df.drop(lat_long_df.loc[lat_long_df['property_long'].isnull()].index, inplace=True)\n",
    "lat_long_df.drop(lat_long_df.loc[lat_long_df['property_lat'].isnull()].index, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cpaige\\AppData\\Local\\Temp\\ipykernel_3620\\1025026994.py:15: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  raw_tech_data = pd.io.sql.read_sql(query_techData,cnxn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nselect distinct ta.branchname, ta.[Primary Home Address - State] as state  ,ta.Worker as tech_name, ta.[Email - Primary Work] as tech_email ,ta.TechZipCode as tech_zip, ZipShp.LAT as tech_lat ,ZipShp.lon as tech_long\nfrom IRSPublish..tv_technicianAddress ta\njoin thirdpartydata..Shp_ZipCode_dt ZipShp\non ta.TechZipCode=ZipShp.ZipCode\njoin modeltestbed..msrportfolio msr\non msr.branchname like ta.Branchname+'%'\nand [Still Under Mgmt]='Yes'\njoin thirdpartydata..Shp_Blocks_dt blkShp\non blkShp.fips+blkShp.censusTract+blkshp.BlockGroup=msr.census_block\nwhere ta.branchName = 'St Louis'\n\n': ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'census_block'. (207) (SQLExecDirectW)\")",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mProgrammingError\u001B[0m                          Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2018\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2017\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2018\u001B[0m     cur\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2019\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cur\n",
      "\u001B[1;31mProgrammingError\u001B[0m: ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'census_block'. (207) (SQLExecDirectW)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDatabaseError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [11], line 15\u001B[0m\n\u001B[0;32m      1\u001B[0m query_techData \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124mselect distinct ta.branchname, ta.[Primary Home Address - State] as state  ,ta.Worker as tech_name, ta.[Email - Primary Work] as tech_email ,ta.TechZipCode as tech_zip, ZipShp.LAT as tech_lat ,ZipShp.lon as tech_long\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124mfrom IRSPublish..tv_technicianAddress ta\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \n\u001B[0;32m     13\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m---> 15\u001B[0m raw_tech_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mio\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_techData\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcnxn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m tech_df \u001B[38;5;241m=\u001B[39m raw_tech_data\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# tech_df = tech_df[tech_df['state'] == 'Missouri']\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:564\u001B[0m, in \u001B[0;36mread_sql\u001B[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize)\u001B[0m\n\u001B[0;32m    561\u001B[0m pandas_sql \u001B[38;5;241m=\u001B[39m pandasSQL_builder(con)\n\u001B[0;32m    563\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(pandas_sql, SQLiteDatabase):\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpandas_sql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_query\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43msql\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex_col\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcoerce_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_float\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunksize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    574\u001B[0m     _is_table_name \u001B[38;5;241m=\u001B[39m pandas_sql\u001B[38;5;241m.\u001B[39mhas_table(sql)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2078\u001B[0m, in \u001B[0;36mSQLiteDatabase.read_query\u001B[1;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize, dtype)\u001B[0m\n\u001B[0;32m   2066\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_query\u001B[39m(\n\u001B[0;32m   2067\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2068\u001B[0m     sql,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2074\u001B[0m     dtype: DtypeArg \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2075\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Iterator[DataFrame]:\n\u001B[0;32m   2077\u001B[0m     args \u001B[38;5;241m=\u001B[39m _convert_params(sql, params)\n\u001B[1;32m-> 2078\u001B[0m     cursor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2079\u001B[0m     columns \u001B[38;5;241m=\u001B[39m [col_desc[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m col_desc \u001B[38;5;129;01min\u001B[39;00m cursor\u001B[38;5;241m.\u001B[39mdescription]\n\u001B[0;32m   2081\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\JetBrains\\DataSpell2022.1\\projects\\workspace\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2030\u001B[0m, in \u001B[0;36mSQLiteDatabase.execute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2027\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01minner_exc\u001B[39;00m\n\u001B[0;32m   2029\u001B[0m ex \u001B[38;5;241m=\u001B[39m DatabaseError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecution failed on sql \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00margs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2030\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m ex \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[1;31mDatabaseError\u001B[0m: Execution failed on sql '\nselect distinct ta.branchname, ta.[Primary Home Address - State] as state  ,ta.Worker as tech_name, ta.[Email - Primary Work] as tech_email ,ta.TechZipCode as tech_zip, ZipShp.LAT as tech_lat ,ZipShp.lon as tech_long\nfrom IRSPublish..tv_technicianAddress ta\njoin thirdpartydata..Shp_ZipCode_dt ZipShp\non ta.TechZipCode=ZipShp.ZipCode\njoin modeltestbed..msrportfolio msr\non msr.branchname like ta.Branchname+'%'\nand [Still Under Mgmt]='Yes'\njoin thirdpartydata..Shp_Blocks_dt blkShp\non blkShp.fips+blkShp.censusTract+blkshp.BlockGroup=msr.census_block\nwhere ta.branchName = 'St Louis'\n\n': ('42S22', \"[42S22] [Microsoft][SQL Server Native Client 11.0][SQL Server]Invalid column name 'census_block'. (207) (SQLExecDirectW)\")"
     ]
    }
   ],
   "source": [
    "query_techData = \"\"\"\n",
    "select distinct ta.branchname, ta.[Primary Home Address - State] as state  ,ta.Worker as tech_name, ta.[Email - Primary Work] as tech_email ,ta.TechZipCode as tech_zip, ZipShp.LAT as tech_lat ,ZipShp.lon as tech_long\n",
    "from IRSPublish..tv_technicianAddress ta\n",
    "join thirdpartydata..Shp_ZipCode_dt ZipShp\n",
    "on ta.TechZipCode=ZipShp.ZipCode\n",
    "join modeltestbed..msrportfolio msr\n",
    "on msr.branchname like ta.Branchname+'%'\n",
    "and [Still Under Mgmt]='Yes'\n",
    "join thirdpartydata..Shp_Blocks_dt blkShp\n",
    "on blkShp.fips+blkShp.censusTract+blkshp.BlockGroup=msr.census_block\n",
    "where ta.branchName = 'St Louis'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "raw_tech_data = pd.io.sql.read_sql(query_techData,cnxn)\n",
    "\n",
    "tech_df = raw_tech_data.copy()\n",
    "# tech_df = tech_df[tech_df['state'] == 'Missouri']\n",
    "tech_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# K-Means Clustering Assignments\n",
    "\n",
    "### mini-pod: n_clusters = # of techs\n",
    "### big-pod: n_clusters = 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "miniPod_kmeans = KMeansConstrained(n_clusters=len(tech_df), size_min=round(len(lat_long_df) / len(tech_df)) - 3)\n",
    "\n",
    "lat_long_df['miniPod_id'] = miniPod_kmeans.fit_predict(lat_long_df[['property_lat', 'property_long']])\n",
    "\n",
    "lat_long_df['miniPod_id'] = lat_long_df['miniPod_id'] + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "miniPod_centers = miniPod_kmeans.cluster_centers_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "miniPod_labels = miniPod_kmeans.predict(lat_long_df[['property_lat', 'property_long']])\n",
    "miniPod_labels = miniPod_labels + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat_long_df['miniPod_id'].value_counts().sort_index(ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat_long_df.plot.scatter(x='property_lat', y='property_long', c=miniPod_labels, s=50, cmap= 'seismic')\n",
    "plt.scatter(miniPod_centers[:, 0], miniPod_centers[:, 1], c='black', s=75, alpha=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_clus = math.ceil(len(lat_long_df)/1500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigPod_kmeans = KMeansConstrained(n_clusters=n_clus, size_min=round(len(lat_long_df) / n_clus) - 3)\n",
    "\n",
    "lat_long_df['bigPod_id'] = bigPod_kmeans.fit_predict(lat_long_df[['property_lat', 'property_long']])\n",
    "\n",
    "lat_long_df['bigPod_id'] = lat_long_df['bigPod_id'] + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigPod_centers = bigPod_kmeans.cluster_centers_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bigPod_labels = bigPod_kmeans.predict(lat_long_df[['property_lat', 'property_long']])\n",
    "bigPod_labels = bigPod_labels + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat_long_df['bigPod_id'].value_counts().sort_index(ascending=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat_long_df.plot.scatter(x='property_lat', y='property_long', c=bigPod_labels, s=50, cmap= 'seismic')\n",
    "plt.scatter(bigPod_centers[:, 0], bigPod_centers[:, 1], c='black', s=75, alpha=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = lat_long_df.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def censusTractSplit(collected_df):\n",
    "\n",
    "    collected_df['fips'] = [x[0:5] for x in collected_df['CensusTractGeoId'].astype(str)]\n",
    "\n",
    "    df_assign = collected_df.groupby(['fips','subdivision_name'])['miniPod_id'].nunique().to_frame().reset_index()\n",
    "\n",
    "    df_assign = df_assign[df_assign['miniPod_id']>1]\n",
    "    df_assign = df_assign.rename(columns={'miniPod_id':'nMiniPods'})\n",
    "\n",
    "    N_prop = collected_df.groupby(['fips','subdivision_name','miniPod_id'])['propertyid'].count().to_frame().reset_index()\n",
    "    N_prop = N_prop.rename(columns = {'propertyid':'nProp'})\n",
    "\n",
    "    to_choose = N_prop.merge(df_assign, left_on=['fips','subdivision_name'], right_on= ['fips','subdivision_name'])\n",
    "\n",
    "    to_choose['miniPod_rank'] =  to_choose.groupby(['fips','subdivision_name'])['nProp'].rank(method = 'first',ascending=False)\n",
    "\n",
    "    to_choose = to_choose[to_choose['miniPod_rank']==1][['fips','subdivision_name','miniPod_id']]\n",
    "\n",
    "    #now we just merge back\n",
    "    assigned = collected_df.merge(to_choose,how='left', left_on=['fips','subdivision_name'], right_on=['fips','subdivision_name'])\n",
    "    assigned = assigned.rename(columns={'miniPod_id_x':'prevMiniPodID','miniPod_id_y':'miniPod_id'})\n",
    "\n",
    "    new_assigned_df = assigned[assigned['miniPod_id'].notnull()].drop('prevMiniPodID', axis=1)\n",
    "\n",
    "    lst = list(new_assigned_df['propertyid'])\n",
    "\n",
    "    collected_df.drop(collected_df.loc[collected_df['propertyid'].isin(lst)].index, inplace=True)\n",
    "\n",
    "    collected_df = pd.concat([collected_df, new_assigned_df])\n",
    "\n",
    "    collected_df = collected_df.drop('fips', axis=1)\n",
    "\n",
    "    collected_df['propertyid'] = collected_df['propertyid'].astype(int)\n",
    "    collected_df['miniPod_id'] = collected_df['miniPod_id'].astype(int)\n",
    "    collected_df['bigPod_id'] = collected_df['bigPod_id'].astype(int)\n",
    "\n",
    "\n",
    "    return collected_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = censusTractSplit(collected_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def same_bigPod(df):\n",
    "\n",
    "    df_assign = df.groupby(['miniPod_id'])['bigPod_id'].nunique().to_frame().reset_index().rename(columns={'bigPod_id': 'nBigPods'})\n",
    "\n",
    "    df_assign = df_assign[df_assign['nBigPods']>1]\n",
    "\n",
    "    n_prop = df.groupby(['miniPod_id', 'bigPod_id'])['propertyid'].count().to_frame().reset_index().rename(columns = {'propertyid':'nProp'})\n",
    "\n",
    "    to_choose = n_prop.merge(df_assign, left_on='miniPod_id', right_on='miniPod_id')\n",
    "\n",
    "    to_choose['miniPod_rank'] = to_choose.groupby('miniPod_id')['nProp'].rank(method='first', ascending=False)\n",
    "\n",
    "    to_choose = to_choose[to_choose['miniPod_rank'] ==1][['miniPod_id', 'bigPod_id']]\n",
    "\n",
    "    assigned = df.merge(to_choose, how='left', left_on='miniPod_id', right_on='miniPod_id')\n",
    "    assigned = assigned.rename(columns={'bigPod_id_x':'prevBigPodID','bigPod_id_y':'bigPod_id'})\n",
    "\n",
    "    new_assigned = assigned[assigned['bigPod_id'].notnull()].drop('prevBigPodID', axis=1)\n",
    "\n",
    "    lst = list(new_assigned['propertyid'])\n",
    "\n",
    "    df.drop(df.loc[df['propertyid'].isin(lst)].index, inplace=True)\n",
    "\n",
    "    new_df = new_assigned[['propertyid', 'branchname', 'CensusTractGeoId', 'property_lat', 'property_long', 'subdivision_name', 'miniPod_id', 'bigPod_id']]\n",
    "\n",
    "    final_df = pd.concat([df, new_df])\n",
    "\n",
    "    return final_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = same_bigPod(collected_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_centers():\n",
    "    centers_df = pd.DataFrame(miniPod_centers)\n",
    "    centers_df.rename(columns={0:'miniPod_lat', 1:'miniPod_long'}, inplace=True)\n",
    "\n",
    "    labels_df = pd.DataFrame(set(miniPod_labels))\n",
    "    labels_df = labels_df.rename(columns={0:'clusterLabel'})\n",
    "\n",
    "    centers_df['miniPod_id'] = labels_df['clusterLabel']\n",
    "\n",
    "    centers_df['miniPod_id'] = centers_df['miniPod_id']\n",
    "\n",
    "    return centers_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = collected_df.merge(get_centers(), how='left', on='miniPod_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_bigPod_center():\n",
    "\n",
    "\n",
    "\n",
    "    pod_centers = pd.DataFrame(bigPod_centers)\n",
    "    pod_centers['bigPod_id'] = np.unique(bigPod_labels)\n",
    "    pod_centers.rename(columns={0:'bigPod_lat', 1:'bigPod_long'}, inplace=True)\n",
    "\n",
    "    return pod_centers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = collected_df.merge(get_bigPod_center(), how='left', on='bigPod_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openrouteservice as ors\n",
    "\n",
    "client = ors.Client(base_url='http://localhost:8080/ors')\n",
    "\n",
    "def miniPodcenter2prop(row):\n",
    "    index, value = row\n",
    "    try:\n",
    "        coords = ((value[\"miniPod_long\"],value[\"miniPod_lat\"]),(value['property_long'],value['property_lat']))\n",
    "        routes = client.directions(coords, optimize_waypoints=True ,radiuses=50000)\n",
    "        result = {\n",
    "            'propertyid': value['propertyid'],\n",
    "            'branchname': value['branchname'],\n",
    "            'CensusTractGeoId' : value['CensusTractGeoId'],\n",
    "            'subdivision_name': value['subdivision_name'],\n",
    "            'property_lat': value['property_lat'],\n",
    "            'property_long': value['property_long'],\n",
    "            'miniPod_id': value['miniPod_id'],\n",
    "            'bigPod_id': value['bigPod_id'],\n",
    "            'miniPod_lat':value['miniPod_lat'],\n",
    "            'miniPod_long': value['miniPod_long'],\n",
    "            'bigPod_lat':value['bigPod_lat'],\n",
    "            'bigPod_long': value['bigPod_long'],\n",
    "            'miniPodcenter2prop_driving_distance(meters)': routes['routes'][0]['summary']['distance'],\n",
    "            'miniPodcenter2prop_driving_time(secs)': routes['routes'][0]['summary']['duration']\n",
    "        }\n",
    "    except:\n",
    "        result = {'invalid_row': index}\n",
    "    return result\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "pool = ThreadPool(14)\n",
    "results = pool.map(miniPodcenter2prop, collected_df.iterrows())\n",
    "pool.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = pd.DataFrame([x_ for x_ in results if not x_.get('invalid_row')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bigPododcenter2prop(row):\n",
    "    index, value = row\n",
    "    try:\n",
    "        coords = ((value[\"bigPod_long\"],value[\"bigPod_lat\"]),(value['property_long'],value['property_lat']))\n",
    "        routes = client.directions(coords, optimize_waypoints=True ,radiuses=50000)\n",
    "        result = {\n",
    "            'propertyid': value['propertyid'],\n",
    "            'branchname': value['branchname'],\n",
    "            'CensusTractGeoId' : value['CensusTractGeoId'],\n",
    "            'subdivision_name': value['subdivision_name'],\n",
    "            'property_lat': value['property_lat'],\n",
    "            'property_long': value['property_long'],\n",
    "            'miniPod_id': value['miniPod_id'],\n",
    "            'bigPod_id': value['bigPod_id'],\n",
    "            'miniPod_lat':value['miniPod_lat'],\n",
    "            'miniPod_long': value['miniPod_long'],\n",
    "            'bigPod_lat': value['bigPod_lat'],\n",
    "            'bigPod_long': value['bigPod_long'],\n",
    "            'miniPodcenter2prop_driving_distance(meters)': value['miniPodcenter2prop_driving_distance(meters)'],\n",
    "            'miniPodcenter2prop_driving_time(secs)': value['miniPodcenter2prop_driving_time(secs)'],\n",
    "            'bigPodcenter2prop_driving_distance(meters)': routes['routes'][0]['summary']['distance'],\n",
    "            'bigPodcenter2prop_driving_time(secs)': routes['routes'][0]['summary']['duration']\n",
    "        }\n",
    "    except:\n",
    "        result = {'invalid_row': index}\n",
    "    return result\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "pool = ThreadPool(14)\n",
    "results = pool.map(bigPododcenter2prop, collected_df.iterrows())\n",
    "pool.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = pd.DataFrame([x_ for x_ in results if not x_.get('invalid_row')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tech Assignment and Optimization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech2center = tech_df.merge(get_centers(), how='cross')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tech2center_(row):\n",
    "    index, value = row\n",
    "    try:\n",
    "        coords = ((value[\"tech_long\"],value[\"tech_lat\"]),(value['miniPod_long'],value['miniPod_lat']))\n",
    "        routes = client.directions(coords, optimize_waypoints=True ,radiuses=15000)\n",
    "        result = {\n",
    "            'tech_zip': value['tech_zip'],\n",
    "            'tech_name': value['tech_name'],\n",
    "            'tech_email': value['tech_email'],\n",
    "            'tech_lat': value['tech_lat'],\n",
    "            'tech_long': value['tech_long'],\n",
    "            'miniPod_lat': value['miniPod_lat'],\n",
    "            'miniPod_long': value['miniPod_long'],\n",
    "            'miniPod_id':value['miniPod_id'],\n",
    "            'driving_distance': routes['routes'][0]['summary']['distance'],\n",
    "            'driving_time': routes['routes'][0]['summary']['duration']\n",
    "        }\n",
    "    except:\n",
    "        result = {'invalid_row': index}\n",
    "    return result\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "pool = ThreadPool(14)\n",
    "results = pool.map(tech2center_, tech2center.iterrows())\n",
    "pool.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech2center_df = pd.DataFrame([x_ for x_ in results if not x_.get('invalid_row')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech2center_final = tech2center_df.groupby(['tech_name', 'miniPod_id'])['driving_time'].sum().to_frame().reset_index().sort_values(['tech_name','miniPod_id'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech_assigned = tech2center_final.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech2center_final = tech2center_final.set_index(['miniPod_id','tech_name'],drop = True).unstack('miniPod_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech2center_final.columns = tech2center_final.columns.droplevel(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from ortools.graph.python import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "drivingTime_array = tech2center_final.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Let's transform this into 3 parallel vectors (start_nodes, end_nodes, arc_costs)\n",
    "end_nodes_unraveled, start_nodes_unraveled = np.meshgrid(np.arange(drivingTime_array.shape[1]),np.arange(drivingTime_array.shape[0]))\n",
    "start_nodes = start_nodes_unraveled.ravel()\n",
    "end_nodes = end_nodes_unraveled.ravel()\n",
    "arc_costs = drivingTime_array.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assignment = linear_sum_assignment.SimpleLinearSumAssignment()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assignment.add_arcs_with_cost(start_nodes, end_nodes, arc_costs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "status = assignment.solve()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if status == assignment.OPTIMAL:\n",
    "    print(f'Total driving time for all Techs: {assignment.optimal_cost()}', 'mins\\n')\n",
    "    for i in range(0, assignment.num_nodes()):\n",
    "        print(f'Technician {i} assigned to mini pod: {assignment.right_mate(i)}' +\n",
    "              f'  (Driving Time(mins) = {assignment.assignment_cost(i)})')\n",
    "elif status == assignment.INFEASIBLE:\n",
    "    print('No assignment is possible.')\n",
    "elif status == assignment.POSSIBLE_OVERFLOW:\n",
    "    print(\n",
    "        'Some input costs are too large and may cause an integer overflow.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizedTime_lst = []\n",
    "cluster_lst = []\n",
    "for i in range(0, assignment.num_nodes()):\n",
    "    optimizedTime_lst.append(assignment.assignment_cost(i))\n",
    "    cluster_lst.append(assignment.right_mate(i))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech_assigned = tech_assigned.merge(tech2center_df[['tech_name','tech_lat', 'tech_long', 'miniPod_lat', 'miniPod_long']])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lst = []\n",
    "lst2 = []\n",
    "df = pd.DataFrame()\n",
    "for i in tech_df['tech_name']:\n",
    "    lat = tech_df.loc[tech_df['tech_name'] == i, 'tech_lat'].iloc[0]\n",
    "    long = tech_df.loc[tech_df['tech_name'] == i, 'tech_long'].iloc[0]\n",
    "    lst.append(lat)\n",
    "    lst2.append(long)\n",
    "\n",
    "\n",
    "df['tech_lat'] = lst\n",
    "df['tech_long'] = lst2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizedTime_techAssignment = pd.DataFrame({\n",
    "    'tech_name': list(tech_assigned['tech_name'].unique()),\n",
    "    'tech_lat': list(df['tech_lat']),\n",
    "    'tech_long': list(df['tech_long']),\n",
    "    'miniPod_lat': list(tech_assigned['miniPod_lat'].unique()),\n",
    "    'miniPod_long': list(tech_assigned['miniPod_long'].unique()),\n",
    "    'miniPod_id': cluster_lst,\n",
    "    'optimizedDrivingTimes(mins)': optimizedTime_lst,\n",
    "})\n",
    "\n",
    "optimizedTime_techAssignment['miniPod_id'] = optimizedTime_techAssignment['miniPod_id'] + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech_assignment = optimizedTime_techAssignment.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tech_assigned = tech_assignment.merge(tech_df[['tech_name', 'tech_zip', 'tech_email']], how='left', on='tech_name')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = collected_df.merge(tech_assigned[['miniPod_id','tech_name','tech_email', 'tech_zip', 'tech_lat', 'tech_long']], how='left', on='miniPod_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tech2prop(row):\n",
    "    index, value = row\n",
    "    try:\n",
    "        coords = ((value[\"tech_long\"],value[\"tech_lat\"]),(value['property_long'],value['property_lat']))\n",
    "        routes = client.directions(coords, optimize_waypoints=True ,radiuses=50000)\n",
    "        result = {\n",
    "            'propertyid': value['propertyid'],\n",
    "            'branchname': value['branchname'],\n",
    "            'subdivision_name': value['subdivision_name'],\n",
    "            'property_lat': value['property_lat'],\n",
    "            'property_long': value['property_long'],\n",
    "            'miniPod_id': value['miniPod_id'],\n",
    "            'bigPod_id': value['bigPod_id'],\n",
    "            'miniPod_lat':value['miniPod_lat'],\n",
    "            'miniPod_long': value['miniPod_long'],\n",
    "            'bigPod_lat': value['bigPod_lat'],\n",
    "            'bigPod_long': value['bigPod_long'],\n",
    "            'miniPodcenter2prop_driving_distance(meters)': value['miniPodcenter2prop_driving_distance(meters)'],\n",
    "            'miniPodcenter2prop_driving_time(secs)': value['miniPodcenter2prop_driving_time(secs)'],\n",
    "            'bigPodcenter2prop_driving_distance(meters)': value['bigPodcenter2prop_driving_distance(meters)'],\n",
    "            'bigPodcenter2prop_driving_time(secs)': value['bigPodcenter2prop_driving_time(secs)'],\n",
    "            'tech_name': value['tech_name'],\n",
    "            'tech_zip': value['tech_zip'],\n",
    "            'tech_email': value['tech_email'],\n",
    "            'tech_lat': value['tech_lat'],\n",
    "            'tech_long': value['tech_long'],\n",
    "            'techZip2prop_driving_distance(meters)':routes['routes'][0]['summary']['distance'],\n",
    "            'techZip2prop_driving_time(secs)': routes['routes'][0]['summary']['duration']\n",
    "        }\n",
    "    except:\n",
    "        result = {'invalid_row': index}\n",
    "    return result\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "pool = ThreadPool(14)\n",
    "results = pool.map(tech2prop, collected_df.iterrows())\n",
    "pool.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_df = pd.DataFrame([x_ for x_ in results if not x_.get('invalid_row')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tech2minipod(row):\n",
    "    index, value = row\n",
    "    try:\n",
    "        coords = ((value[\"tech_long\"],value[\"tech_lat\"]),(value['miniPod_long'],value['miniPod_lat']))\n",
    "        routes = client.directions(coords, optimize_waypoints=True ,radiuses=50000)\n",
    "        result = {\n",
    "            'propertyid': value['propertyid'],\n",
    "            'branchname': value['branchname'],\n",
    "            'subdivision_name': value['subdivision_name'],\n",
    "            'property_latitude': value['property_lat'],\n",
    "            'property_longitude': value['property_long'],\n",
    "            'miniPod_id': value['miniPod_id'],\n",
    "            'bigPod_id': value['bigPod_id'],\n",
    "            'miniPod_lat':value['miniPod_lat'],\n",
    "            'miniPod_long': value['miniPod_long'],\n",
    "            'bigPod_lat': value['bigPod_lat'],\n",
    "            'bigPod_long': value['bigPod_long'],\n",
    "            'miniPod2prop_driving_distance(meters)': value['miniPodcenter2prop_driving_distance(meters)'],\n",
    "            'miniPod2prop_driving_time(secs)': value['miniPodcenter2prop_driving_time(secs)'],\n",
    "            'bigPod2prop_driving_distance(meters)': value['bigPodcenter2prop_driving_distance(meters)'],\n",
    "            'bigPod2prop_driving_time(secs)': value['bigPodcenter2prop_driving_time(secs)'],\n",
    "            'tech_name': value['tech_name'],\n",
    "            'tech_zip': value['tech_zip'],\n",
    "            'tech_email': value['tech_email'],\n",
    "            'tech_lat': value['tech_lat'],\n",
    "            'tech_long': value['tech_long'],\n",
    "            'techZip2prop_driving_distance(meters)':value['techZip2prop_driving_distance(meters)'],\n",
    "            'techZip2prop_driving_time(secs)': value['techZip2prop_driving_time(secs)'],\n",
    "            'techZip2miniPod_driving_distance(meters)':routes['routes'][0]['summary']['distance'],\n",
    "            'techZip2miniPod_driving_time(secs)': routes['routes'][0]['summary']['duration']\n",
    "        }\n",
    "    except:\n",
    "        result = {'invalid_row': index}\n",
    "    return result\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "pool = ThreadPool(14)\n",
    "results = pool.map(tech2minipod, collected_df.iterrows())\n",
    "pool.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_final = pd.DataFrame([x_ for x_ in results if not x_.get('invalid_row')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_final = collected_final[['propertyid',\n",
    "                                    'branchname',\n",
    "                                    'subdivision_name',\n",
    "                                    'property_latitude',\n",
    "                                    'property_longitude',\n",
    "                                    'tech_lat',\n",
    "                                    'tech_long',\n",
    "                                    'bigPod_lat',\n",
    "                                    'bigPod_long',\n",
    "                                    'miniPod_lat',\n",
    "                                    'miniPod_long',\n",
    "                                    'miniPod_id',\n",
    "                                    'bigPod_id',\n",
    "                                    'bigPod2prop_driving_distance(meters)',\n",
    "                                    'bigPod2prop_driving_time(secs)',\n",
    "                                    'miniPod2prop_driving_distance(meters)',\n",
    "                                    'miniPod2prop_driving_time(secs)',\n",
    "                                    'techZip2prop_driving_distance(meters)',\n",
    "                                    'techZip2prop_driving_time(secs)',\n",
    "                                    'techZip2miniPod_driving_distance(meters)',\n",
    "                                    'techZip2miniPod_driving_time(secs)',\n",
    "                                    'tech_zip',\n",
    "                                    'tech_name',\n",
    "                                    'tech_email'\n",
    "                                   ]]\n",
    "\n",
    "collected_final['bigPod_id'] = collected_final['bigPod_id'].astype(int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_final.groupby(['tech_name', 'miniPod_id']).agg({'techZip2miniPod_driving_time(secs)':'mean'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "collected_final"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# collected_final.to_csv('stLouis_Assigned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}